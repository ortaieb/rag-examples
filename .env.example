# =============================================================================
# RAG Examples Environment Configuration
# =============================================================================
# Copy this file to .env and modify the values as needed
# cp env.example .env

# =============================================================================
# Vector Store Configuration
# =============================================================================

# Database location for Chroma vector store
# Path where the Chroma vector database will be stored
VECTOR_DB_LOCATION=./chroma_store_db

# Number of documents to retrieve when searching the vector store
# Controls how many reviews are provided to the LLM for context
VECTOR_K_VALUE=5

# =============================================================================
# LLM Configuration
# =============================================================================

# Model temperature (0.0 to 1.0)
# Controls randomness in model responses
# 0.0 = deterministic, 1.0 = very random
TEMPERATURE=0.1

# Maximum tokens for responses
# Maximum number of tokens in the model's response
MAX_TOKENS=1000

# =============================================================================
# CLAUDE Configuration
# =============================================================================

# Model to be used in Claude
CLAUDE_MODEL=claude-3-opus-20240229

# ANTHROPIC identification
# !!! Remember - It is not recommended to share API KEY or keep it as cleartext
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# =============================================================================
# Example Customizations
# =============================================================================
# Uncomment and modify these lines to customize your setup:

# Use a different vector store location
# VECTOR_DB_LOCATION=./my_custom_chroma_store

# Retrieve more reviews for better context
# VECTOR_K_VALUE=10

# Use a different embedding model
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Use a different LLM model
# OLLAMA_LLM_MODEL=qwen2.5:14b

# Increase creativity in responses
# TEMPERATURE=0.7

# Allow longer responses
# MAX_TOKENS=2000
