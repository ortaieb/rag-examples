# =============================================================================
# RAG Examples Environment Configuration
# =============================================================================
# Copy this file to .env and modify the values as needed
# cp env.example .env

# =============================================================================
# Vector Store Configuration
# =============================================================================

# Database location for Chroma vector store
# Path where the Chroma vector database will be stored
VECTOR_DB_LOCATION=./chroma_store_db

# Number of documents to retrieve when searching the vector store
# Controls how many reviews are provided to the LLM for context
VECTOR_K_VALUE=5

# =============================================================================
# Ollama Model Configuration
# =============================================================================

# Embedding model for vector operations
# Used for creating embeddings of reviews and queries
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large

# LLM model for chat operations
# Used for generating responses to questions
OLLAMA_LLM_MODEL=llama3.2

# =============================================================================
# LLM Configuration
# =============================================================================

# Model temperature (0.0 to 1.0)
# Controls randomness in model responses
# 0.0 = deterministic, 1.0 = very random
TEMPERATURE=0.1

# Maximum tokens for responses
# Maximum number of tokens in the model's response
MAX_TOKENS=1000

# =============================================================================
# Example Customizations
# =============================================================================
# Uncomment and modify these lines to customize your setup:

# Use a different vector store location
# VECTOR_DB_LOCATION=./my_custom_chroma_store

# Retrieve more reviews for better context
# VECTOR_K_VALUE=10

# Use a different embedding model
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Use a different LLM model
# OLLAMA_LLM_MODEL=qwen2.5:14b

# Increase creativity in responses
# TEMPERATURE=0.7

# Allow longer responses
# MAX_TOKENS=2000
